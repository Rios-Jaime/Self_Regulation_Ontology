{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.5 - imports and read-ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import sklearn\n",
    "import sklearn.decomposition as skdec\n",
    "import matplotlib.pyplot as plt\n",
    "import factor_analyzer\n",
    "import random\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "from utils.clean_utils import transform_remove_skew, remove_outliers, remove_correlated_task_variables, drop_vars\n",
    "\n",
    "from utils.r_to_py_utils import missForest\n",
    "\n",
    "from utils.EFA_assumption_utils import check_sample_size, bartlett_sphericity, kmo\n",
    "\n",
    "from utils.EFA_HCA_utils import Results\n",
    "\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_variables = pd.read_csv('HDDM_meaningful_variables.csv', index_col='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Clean & impute the data\n",
    "\n",
    "__selected_variables_clean = transform_remove_skew(selected_variables)__\n",
    " - for positively skewed (skew > 1) vars: \n",
    "  - shift so min is 0 (or 1?), then positive_subset = log(shifted(var)). remove outliers of positive subset.\n",
    "  - keep those w/ new_skew < thresh, drop those are still too skewed\n",
    " - for negatively skewed (skew < 1) vars: \n",
    "  - negative subset = log(negative_subset.max()+1 - negative_subset) \n",
    "\n",
    "__selected_variables_clean = remove_outliers(selected_variables_clean)__\n",
    " - Removes outliers more than 1.5IQR below Q1 or above Q3\n",
    " \n",
    "__selected_variables_clean = remove_correlated_task_variables(selected_variables_clean)__\n",
    " - Removes variables with corr > 0.85\n",
    "   - removes both correlated vars?\n",
    "\n",
    "__impute data__\n",
    "- missForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "** Successfully transformed 26 positively skewed variables:\n",
      "adaptive_n_back.mean_load\n",
      "angling_risk_task_always_sunny.release_coef_of_variation\n",
      "bickel_titrator.hyp_discount_rate_large\n",
      "bickel_titrator.hyp_discount_rate_medium\n",
      "bickel_titrator.hyp_discount_rate_small\n",
      "bis11_survey.Motor\n",
      "choice_reaction_time.hddm_non_decision\n",
      "columbia_card_task_cold.gain_sensitivity\n",
      "columbia_card_task_hot.gain_sensitivity\n",
      "directed_forgetting.proactive_interference_hddm_drift\n",
      "dospert_eb_survey.health_safety\n",
      "dot_pattern_expectancy.hddm_thresh\n",
      "holt_laury_survey.beta\n",
      "kirby.hyp_discount_rate_large\n",
      "kirby.hyp_discount_rate_medium\n",
      "kirby.hyp_discount_rate_small\n",
      "motor_selective_stop_signal.hddm_thresh\n",
      "shape_matching.hddm_thresh\n",
      "shift_task.model_beta\n",
      "simple_reaction_time.avg_rt\n",
      "stim_selective_stop_signal.hddm_thresh\n",
      "stop_signal.SSRT_high\n",
      "stop_signal.hddm_thresh\n",
      "stroop.hddm_thresh\n",
      "threebytwo.hddm_thresh\n",
      "tower_of_london.avg_move_time\n",
      "****************************************\n",
      "****************************************\n",
      "Dropping 2 positively skewed data that could not be transformed successfully:\n",
      "impulsive_venture_survey.impulsiveness\n",
      "dickman_survey.dysfunctional\n",
      "****************************************\n",
      "****************************************\n",
      "** Successfully transformed 11 negatively skewed variables:\n",
      "attention_network_task.conflict_hddm_drift\n",
      "attention_network_task.hddm_non_decision\n",
      "columbia_card_task_hot.loss_sensitivity\n",
      "motor_selective_stop_signal.hddm_non_decision\n",
      "mpq_control_survey.control\n",
      "selection_optimization_compensation_survey.optimization\n",
      "shape_matching.hddm_non_decision\n",
      "shift_task.model_decay\n",
      "stim_selective_stop_signal.hddm_non_decision\n",
      "stop_signal.hddm_non_decision\n",
      "ten_item_personality_survey.conscientiousness\n",
      "****************************************\n",
      "****************************************\n",
      "Dropping 1 negatively skewed data that could not be transformed successfully:\n",
      "holt_laury_survey.prob_weighting\n",
      "****************************************\n",
      "**************************************************\n",
      "Dropping 8 variables with correlations above 0.85\n",
      "**************************************************\n",
      "angling_risk_task_always_sunny.keep_score\n",
      "angling_risk_task_always_sunny.release_score\n",
      "kirby.percent_patient_small\n",
      "kirby.percent_patient_medium\n",
      "kirby.percent_patient\n",
      "kirby.percent_patient_large\n",
      "kirby.hyp_discount_rate_medium.logTr\n",
      "probabilistic_selection.value_sensitivity\n"
     ]
    }
   ],
   "source": [
    "selected_variables_clean = transform_remove_skew(selected_variables)\n",
    "selected_variables_clean = remove_outliers(selected_variables_clean)\n",
    "selected_variables_clean = remove_correlated_task_variables(selected_variables_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  missForest iteration 1 in progress...done!\n",
      "  missForest iteration 2 in progress...done!\n",
      "  missForest iteration 3 in progress...done!\n"
     ]
    }
   ],
   "source": [
    "selected_variables_imputed, error = missForest(selected_variables_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### ONLY REQUIRED FOR THE SRO DATA - GRAB JUST TASK VARS\n",
    "\n",
    "task_df = drop_vars(selected_variables_imputed, ['survey'], saved_vars = ['holt','cognitive_reflection'])\n",
    "task_df_no_impute = drop_vars(selected_variables_clean, ['survey'], saved_vars = ['holt','cognitive_reflection'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Test assumptions for EFA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sample size (participants:variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good enough for a pilot\n"
     ]
    }
   ],
   "source": [
    "sample_ratio = check_sample_size(task_df)\n",
    "# ncases, nvars = task_data.shape\n",
    "# sample_ratio = ncases / nvars\n",
    "# if sample_ratio > 3:\n",
    "#     print('good enough for a pilot')\n",
    "# if sample_ratio >= 5:\n",
    "#     print('reasonable for a full analysis, but not ideal')\n",
    "# if sample_ratio >= 20:\n",
    "#     print('good enough to publish with!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. bartlett test, want p < 0.5 (or 0.1, or 0.01 ...)\n",
    "\n",
    "from https://www.statisticshowto.datasciencecentral.com/bartletts-test/#BTs :\n",
    "\n",
    "Bartlettâ€™s test for Sphericity compares your correlation matrix (a matrix of Pearson correlations) to the identity matrix. In other words, it checks if there is a redundancy between variables that can be summarized with some factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32922.576204860205, 0.0)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_square_value,p=factor_analyzer.calculate_bartlett_sphericity(task_df)\n",
    "chi_square_value, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "756002.2468745925\n",
      "0.0\n",
      "12086.164338026812\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "chi_square_value1, p1 = ss.bartlett(*[task_df[col].values for col in task_df.columns])\n",
    "print(chi_square_value1)\n",
    "print(p1)\n",
    "chi_square_value2, p2 = ss.bartlett(*task_df.values)\n",
    "print(chi_square_value2)\n",
    "print(p2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33755.89865691212 8256.0 0.0\n",
      "32804.5426116459 8256.0 0.0\n",
      "pearson = same as the factor analyzer!\n"
     ]
    }
   ],
   "source": [
    "chi2_spear,ddl_spear,pvalue_spear = bartlett_sphericity(task_df, corr_method=\"spearman\")\n",
    "print(chi2_spear,ddl_spear,pvalue_spear )\n",
    "chi2_son,ddl_son,pvalue_son = bartlett_sphericity(task_df, corr_method=\"pearson\")\n",
    "print(chi2_son,ddl_son,pvalue_son)\n",
    "print('pearson = same as the factor analyzer!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Kaiser-Meyer-Olkin (KMO) Test, want a score >= 0.6\n",
    "\n",
    "from https://www.statisticshowto.datasciencecentral.com/kaiser-meyer-olkin/ :\n",
    "\n",
    "Kaiser-Meyer-Olkin (KMO) Test is a measure of how suited your data is for Factor Analysis. The test measures sampling adequacy for each variable in the model and for the complete model. The statistic is a measure of the proportion of variance among variables that might be common variance. The lower the proportion, the more suited your data is to Factor Analysis.\n",
    "\n",
    "KMO returns values between 0 and 1. A rule of thumb for interpreting the statistic:\n",
    "\n",
    "KMO values between 0.8 and 1 indicate the sampling is adequate.\n",
    "\n",
    "KMO values less than 0.6 indicate the sampling is not adequate and that remedial action should be taken. Some authors put this value at 0.5, so use your own judgment for values between 0.5 and 0.6.\n",
    "\n",
    "KMO Values close to zero means that there are large partial correlations compared to the sum of correlations. In other words, there are widespread correlations which are a large problem for factor analysis\n",
    "\n",
    "For reference, Kaiser put the following values on the results:\n",
    "\n",
    "* 0.00 to 0.49 unacceptable.\n",
    "* 0.50 to 0.59 miserable.\n",
    "* 0.60 to 0.69 mediocre.\n",
    "* 0.70 to 0.79 middling.\n",
    "* 0.80 to 0.89 meritorious.\n",
    "* 0.90 to 1.00 marvelous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrymj/anaconda3/lib/python3.7/site-packages/factor_analyzer/utils.py:248: UserWarning: The inverse of the variance-covariance matrix was calculated using the Moore-Penrose generalized matrix inversion, due to its determinant being at or very close to zero.\n",
      "  warnings.warn('The inverse of the variance-covariance matrix '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.748513534160494"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmo_all,kmo_model=factor_analyzer.calculate_kmo(task_df)\n",
    "kmo_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7607458858956919\n",
      "0.7492347745616416\n",
      "pearson = same as the factor analyzer!\n"
     ]
    }
   ],
   "source": [
    "dataset_corr_spear = task_df.corr(method=\"spearman\")\n",
    "value_spear,per_variable_spear = kmo(dataset_corr_spear)\n",
    "print(value_spear)\n",
    "\n",
    "dataset_corr_son = task_df.corr(method=\"pearson\")\n",
    "value_son,per_variable_son = kmo(dataset_corr_son)\n",
    "print(value_son)\n",
    "print('pearson = same as the factor analyzer!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Perform EFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48194\n"
     ]
    }
   ],
   "source": [
    "verbose=True\n",
    "bootstrap=False\n",
    "\n",
    "boot_iter = 1000 \n",
    "ID = str(random.getrandbits(16)) \n",
    "print(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = Results(data=task_df,\n",
    "                  data_no_impute = task_df_no_impute,\n",
    "                  dist_metric='abscorrelation',\n",
    "                  name='task',\n",
    "                  filter_regex='task',\n",
    "                  boot_iter=boot_iter,\n",
    "                  ID=ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*******************************************************************************\n",
      "Running EFA, rotate: oblimin\n",
      "*******************************************************************************\n",
      "Is the data adequate for factor analysis? Yes\n",
      "Determining Optimal Dimensionality\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Loading required namespace: GPArotation\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Components:  {'c_metric-BIC': 5}\n",
      "Creating Factor Tree\n",
      "No 5 factor solution computed yet! Computing...\n",
      "Determining Higher Order Factors\n",
      "# of components not specified, using BIC determined #\n",
      "*******************************************************************************\n",
      "Running HCA\n",
      "*******************************************************************************\n",
      "Clustering data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: Error in if (is.na(n) || n > 65536L) stop(\"size cannot be NA nor exceed 65536\") : \n",
      "  missing value where TRUE/FALSE needed\n",
      "Calls: <Anonymous>\n",
      "\n",
      "R[write to console]: In addition: \n",
      "R[write to console]: Warning messages:\n",
      "\n",
      "R[write to console]: 1: \n",
      "R[write to console]: In randomForest.default(x = obsX, y = obsY, ntree = ntree, mtry = mtry,  :\n",
      "R[write to console]: \n",
      " \n",
      "R[write to console]:  The response has five or fewer unique values.  Are you sure you want to do regression?\n",
      "\n",
      "R[write to console]: 2: \n",
      "R[write to console]: In randomForest.default(x = obsX, y = obsY, ntree = ntree, mtry = mtry,  :\n",
      "R[write to console]: \n",
      " \n",
      "R[write to console]:  The response has five or fewer unique values.  Are you sure you want to do regression?\n",
      "\n",
      "R[write to console]: 3: \n",
      "R[write to console]: In randomForest.default(x = obsX, y = obsY, ntree = ntree, mtry = mtry,  :\n",
      "R[write to console]: \n",
      " \n",
      "R[write to console]:  The response has five or fewer unique values.  Are you sure you want to do regression?\n",
      "\n",
      "R[write to console]: 4: \n",
      "R[write to console]: In randomForest.default(x = obsX, y = obsY, ntree = ntree, mtry = mtry,  :\n",
      "R[write to console]: \n",
      " \n",
      "R[write to console]:  The response has five or fewer unique values.  Are you sure you want to do regression?\n",
      "\n",
      "R[write to console]: 5: \n",
      "R[write to console]: In randomForest.default(x = obsX, y = obsY, ntree = ntree, mtry = mtry,  :\n",
      "R[write to console]: \n",
      " \n",
      "R[write to console]:  The response has five or fewer unique values.  Are you sure you want to do regression?\n",
      "\n",
      "R[write to console]: 6: \n",
      "R[write to console]: In randomForest.default(x = obsX, y = obsY, ntree = ntree, mtry = mtry,  :\n",
      "R[write to console]: \n",
      " \n",
      "R[write to console]:  The response has five or fewer unique values.  Are you sure you want to do regression?\n",
      "\n"
     ]
    },
    {
     "ename": "RRuntimeError",
     "evalue": "Error in if (is.na(n) || n > 65536L) stop(\"size cannot be NA nor exceed 65536\") : \n  missing value where TRUE/FALSE needed\nCalls: <Anonymous>\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRRuntimeError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-2f552f483913>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     results.run_clustering_analysis(rotate=rotate, \n\u001b[1;32m      6\u001b[0m                                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                     run_graphs=False)\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#     c = results.EFA.get_c()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     # name factors and clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EFA/utils/EFA_HCA_utils.py\u001b[0m in \u001b[0;36mrun_clustering_analysis\u001b[0;34m(self, cluster_EFA, run_graphs, rotate, dist_metric, verbose)\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdist_metric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m             self.HCA.run(self.data, self.EFA, cluster_EFA=cluster_EFA,\n\u001b[0;32m--> 611\u001b[0;31m                          rotate=rotate, run_graphs=run_graphs, verbose=verbose)\n\u001b[0m\u001b[1;32m    612\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m             \u001b[0mHCA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mHCA_Analysis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_metric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdist_metric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EFA/utils/EFA_HCA_utils.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, data, EFA, cluster_EFA, rotate, run_graphs, verbose)\u001b[0m\n\u001b[1;32m    465\u001b[0m             run_graphs=False, verbose=False):\n\u001b[1;32m    466\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Clustering data\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 467\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    468\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcluster_EFA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    469\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Clustering EFA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EFA/utils/EFA_HCA_utils.py\u001b[0m in \u001b[0;36mcluster_data\u001b[0;34m(self, data, dist_metric, method)\u001b[0m\n\u001b[1;32m    374\u001b[0m             \u001b[0mlabel_append\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'_dist-%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mdist_metric\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         output = hierarchical_cluster(data.T, method=method,\n\u001b[0;32m--> 376\u001b[0;31m                                       pdist_kws={'metric': dist_metric})\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlabel_append\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/EFA/utils/utils.py\u001b[0m in \u001b[0;36mhierarchical_cluster\u001b[0;34m(df, compute_dist, pdist_kws, method, min_cluster_size, cluster_kws)\u001b[0m\n\u001b[1;32m    165\u001b[0m                        \u001b[0;34m'verbose'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m                        'pamStage': False}\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdynamicTreeCut\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'hybrid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;34m**\u001b[0m\u001b[0mcluster_kws\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreorder_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlink\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     return {'linkage': link, \n",
      "\u001b[0;32m~/Documents/EFA/utils/r_to_py_utils.py\u001b[0m in \u001b[0;36mdynamicTreeCut\u001b[0;34m(distance_df, func, method, **cluster_kws)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mdynamicTreeCut\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dynamicTreeCut'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mlink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhclust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'hybrid'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rpy2/robjects/functions.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    190\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr_k\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         return (super(SignatureTranslatedFunction, self)\n\u001b[0;32m--> 192\u001b[0;31m                 .__call__(*args, **kwargs))\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rpy2/robjects/functions.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mnew_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy2rpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFunction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnew_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mnew_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrpy2py\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rpy2/rinterface_lib/conversion.py\u001b[0m in \u001b[0;36m_\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_cdata_res_to_rinterface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mcdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0;31m# TODO: test cdata is of the expected CType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_cdata_to_rinterface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rpy2/rinterface.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m                     error_occured))\n\u001b[1;32m    788\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0merror_occured\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_rinterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_geterrmessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRRuntimeError\u001b[0m: Error in if (is.na(n) || n > 65536L) stop(\"size cannot be NA nor exceed 65536\") : \n  missing value where TRUE/FALSE needed\nCalls: <Anonymous>\n"
     ]
    }
   ],
   "source": [
    "for rotate in ['oblimin', 'varimax']:\n",
    "    results.run_EFA_analysis(rotate=rotate, \n",
    "                             verbose=verbose, \n",
    "                             bootstrap=bootstrap)\n",
    "    results.run_clustering_analysis(rotate=rotate, \n",
    "                                    verbose=verbose, \n",
    "                                    run_graphs=False)\n",
    "#     c = results.EFA.get_c()\n",
    "#     # name factors and clusters\n",
    "#     factor_names = subset.get('%s_factor_names' % rotate, None)\n",
    "#     cluster_names = subset.get('%s_cluster_names' % rotate, None)\n",
    "#     if factor_names:\n",
    "#         results.EFA.name_factors(factor_names, rotate=rotate)\n",
    "#     if cluster_names:\n",
    "#         results.HCA.name_clusters(cluster_names, inp='EFA%s_%s' % (c, rotate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_EFA=True\n",
    "run_graphs=False\n",
    "rotate='oblimin'\n",
    "verbose=True\n",
    "dist_metric=None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if verbose:\n",
    "    print('*'*79)\n",
    "    print('Running HCA')\n",
    "    print('*'*79)\n",
    "if dist_metric is None: \n",
    "    data=task_df\n",
    "    self_EFA = results.EFA\n",
    "    cluster_EFA = cluster_EFA\n",
    "    rotate=rotate\n",
    "    run_graphs=run_graphs\n",
    "    verbose=verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if verbose: print(\"Clustering data\")\n",
    "        data=data\n",
    "        dist_metric=None\n",
    "        method='average'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.utils import distcorr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        if dist_metric is None:\n",
    "            dist_metric = distcorr\n",
    "            label_append = ''\n",
    "        df = data.T\n",
    "        method=method\n",
    "        pdist_kws={'metric': dist_metric}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size=3\n",
    "compute_dist=True\n",
    "cluster_kws=None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import leaves_list, linkage, cut_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # if compute_dist = False, assume df is a distance matrix. Otherwise\n",
    "    # compute distance on df rows\n",
    "    if compute_dist == True:\n",
    "        if pdist_kws is None:\n",
    "            pdist_kws= {'metric': 'correlation'}\n",
    "        if pdist_kws['metric'] == 'abscorrelation':\n",
    "            # convert to absolute correlations\n",
    "            dist_vec = abs_pdist(df)\n",
    "        elif pdist_kws['metric'] == 'sqcorrelation':\n",
    "            # convert to squared correlations\n",
    "            dist_vec = squareform(1-df.T.corr()**2)\n",
    "        else:\n",
    "            dist_vec = pdist(df, **pdist_kws)\n",
    "        dist_df = pd.DataFrame(squareform(dist_vec), \n",
    "                               index=df.index, \n",
    "                               columns=df.index)\n",
    "    else:\n",
    "        assert df.shape[0] == df.shape[1]\n",
    "        dist_df = df\n",
    "        dist_vec = squareform(df.values)\n",
    "    #clustering. This works the same as hclust\n",
    "    link = linkage(dist_vec, method=method)    \n",
    "    #dendrogram\n",
    "    # same as order.dendrogram(as.dendrogram(hclust output)) in R\n",
    "    reorder_vec = leaves_list(link)\n",
    "    clustered_df = dist_df.iloc[reorder_vec, reorder_vec]\n",
    "    # clustering\n",
    "    if cluster_kws is None:\n",
    "        cluster_kws = {'minClusterSize': 3,\n",
    "                       'verbose': 0,\n",
    "                       'pamStage': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dist_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func='hybrid'\n",
    "method=method\n",
    "minClusterSize = 3\n",
    "verbose = 0\n",
    "pamStage = False\n",
    "\n",
    "distance_df = dist_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    stats = importr('stats')\n",
    "    dynamicTreeCut = importr('dynamicTreeCut')\n",
    "    dist = stats.as_dist(distance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2\n",
    "dist = rpy2.robjects.r['dist'](distance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argwhere(np.isnan(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    link = stats.hclust(dist.tolist(), method=method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(stats.as_dist(distance_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def run_clustering_analysis(self, cluster_EFA=True, run_graphs=True,\n",
    "                                rotate='oblimin',  dist_metric=None, verbose=False):\n",
    "        \"\"\" Run HCA Analysis\n",
    "        \n",
    "        Args:\n",
    "            dist_metric: if provided, create a new HCA instances with the\n",
    "                provided dist_metric and return it. If None (default) run\n",
    "                the results' internal HCA with the dist_metric provided\n",
    "                at creation\n",
    "        \"\"\"\n",
    "        if verbose:\n",
    "            print('*'*79)\n",
    "            print('Running HCA')\n",
    "            print('*'*79)\n",
    "        if dist_metric is None: \n",
    "            self.HCA.run(self.data, self.EFA, cluster_EFA=cluster_EFA,\n",
    "                         rotate=rotate, run_graphs=run_graphs, verbose=verbose)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def run(self, data, EFA, cluster_EFA=False, rotate='oblimin',\n",
    "            run_graphs=False, verbose=False):\n",
    "        if verbose: print(\"Clustering data\")\n",
    "        self.cluster_data(data)\n",
    "        if cluster_EFA:\n",
    "            if verbose: print(\"Clustering EFA\")\n",
    "            self.cluster_EFA(EFA, EFA.get_c(),\n",
    "                             rotate=rotate)\n",
    "#         if run_graphs == True:\n",
    "#             # run graph analysis on raw data\n",
    "#             graphs = self.build_graphs('data', data)\n",
    "#             self.results['data']['graphs'] = graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    def cluster_data(self, data, dist_metric=None, method='average'):\n",
    "        if dist_metric is None:\n",
    "            dist_metric = self.dist_metric\n",
    "            label_append = ''\n",
    "        else:\n",
    "            label_append = '_dist-%s' % dist_metric\n",
    "        output = hierarchical_cluster(data.T, method=method,\n",
    "                                      pdist_kws={'metric': dist_metric})\n",
    "        self.results['data%s' % label_append] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_cluster(df, compute_dist=True,  pdist_kws=None, \n",
    "                         method='average', min_cluster_size=3,\n",
    "                         cluster_kws=None):\n",
    "    \"\"\"\n",
    "    plot hierarchical clustering and heatmap\n",
    "    :df: a correlation matrix\n",
    "    parse_heatmap: int (optional). If defined, devides the columns of the \n",
    "                    heatmap based on cutting the dendrogram\n",
    "    \"\"\"\n",
    "    \n",
    "    # if compute_dist = False, assume df is a distance matrix. Otherwise\n",
    "    # compute distance on df rows\n",
    "    if compute_dist == True:\n",
    "        if pdist_kws is None:\n",
    "            pdist_kws= {'metric': 'correlation'}\n",
    "        if pdist_kws['metric'] == 'abscorrelation':\n",
    "            # convert to absolute correlations\n",
    "            dist_vec = abs_pdist(df)\n",
    "        elif pdist_kws['metric'] == 'sqcorrelation':\n",
    "            # convert to squared correlations\n",
    "            dist_vec = squareform(1-df.T.corr()**2)\n",
    "        else:\n",
    "            dist_vec = pdist(df, **pdist_kws)\n",
    "        dist_df = pd.DataFrame(squareform(dist_vec), \n",
    "                               index=df.index, \n",
    "                               columns=df.index)\n",
    "    else:\n",
    "        assert df.shape[0] == df.shape[1]\n",
    "        dist_df = df\n",
    "        dist_vec = squareform(df.values)\n",
    "    #clustering. This works the same as hclust\n",
    "    link = linkage(dist_vec, method=method)    \n",
    "    #dendrogram\n",
    "    # same as order.dendrogram(as.dendrogram(hclust output)) in R\n",
    "    reorder_vec = leaves_list(link)\n",
    "    clustered_df = dist_df.iloc[reorder_vec, reorder_vec]\n",
    "    # clustering\n",
    "    if cluster_kws is None:\n",
    "        cluster_kws = {'minClusterSize': 3,\n",
    "                       'verbose': 0,\n",
    "                       'pamStage': False}\n",
    "    labels = dynamicTreeCut(dist_df, func='hybrid', method=method,  **cluster_kws)\n",
    "    labels = reorder_labels(labels, link)\n",
    "    return {'linkage': link, \n",
    "            'distance_df': dist_df, \n",
    "            'clustered_df': clustered_df,\n",
    "            'reorder_vec': reorder_vec,\n",
    "            'labels': labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamicTreeCut(distance_df, func='hybrid', method='average', **cluster_kws):\n",
    "    \"\"\" uses DynamicTreeCut to find clusters\n",
    "    Args:\n",
    "        method = \"hybrid\" or \"dyanmic\":\n",
    "    \"\"\"\n",
    "    stats = importr('stats')\n",
    "    dynamicTreeCut = importr('dynamicTreeCut')\n",
    "    dist = stats.as_dist(distance_df)\n",
    "    link = stats.hclust(dist, method=method)\n",
    "    if func == 'hybrid':\n",
    "        dist = stats.as_dist(distance_df)\n",
    "        clustering = dynamicTreeCut.cutreeHybrid(link, distance_df, **cluster_kws)\n",
    "        return np.array(clustering[0])\n",
    "    elif func == 'dynamic':\n",
    "        clustering = dynamicTreeCut.cutreeDynamic(link, **cluster_kws)\n",
    "        return np.array(clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
