{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrymj/anaconda3/lib/python3.7/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: During startup - \n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/henrymj/anaconda3/lib/python3.7/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: Warning messages:\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/henrymj/anaconda3/lib/python3.7/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: 1: package ‘methods’ was built under R version 3.6.3 \n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/henrymj/anaconda3/lib/python3.7/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: 2: package ‘datasets’ was built under R version 3.6.3 \n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/henrymj/anaconda3/lib/python3.7/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: 3: package ‘utils’ was built under R version 3.6.3 \n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/henrymj/anaconda3/lib/python3.7/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: 4: package ‘grDevices’ was built under R version 3.6.3 \n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/henrymj/anaconda3/lib/python3.7/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: 5: package ‘graphics’ was built under R version 3.6.3 \n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/Users/henrymj/anaconda3/lib/python3.7/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: 6: package ‘stats’ was built under R version 3.6.3 \n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as ss\n",
    "import sklearn\n",
    "import sklearn.decomposition as skdec\n",
    "import matplotlib.pyplot as plt\n",
    "import factor_analyzer\n",
    "from rpy2.robjects.packages import importr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_df = pd.read_csv('taskdata_imputed.csv')\n",
    "del task_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Ian did:\n",
    "1 select variables, choose from:\n",
    "- noDDM\n",
    "- EZ ddm\n",
    "- hddm\n",
    "\n",
    "2 clean variables * NOTE THIS IS BEING DONE FULL DATASET (task + survey) *\n",
    "- transform_remove_skew\n",
    " - for positively skewed (skew > 1) vars: \n",
    "  - shift so min is 0 (or 1?), then positive_subset = log(shifted(var)). remove outliers of positive subset.\n",
    "  - keep those w/ new_skew < thresh, drop those are still too skewed\n",
    " - for negatively skewed (skew < 1) vars: \n",
    "  - negative subset = log(negative_subset.max()+1 - negative_subset) \n",
    "  - I believe this creates a right skewed dist with a min 1(?) then takes the log\n",
    "  - then remove outliers, keep successful transforms, drop failures\n",
    "- remove_outliers\n",
    "- remove_correlated_task_variables ????\n",
    "\n",
    "selected_variables_clean = transform_remove_skew(selected_variables)\n",
    "\n",
    "selected_variables_clean = remove_outliers(selected_variables_clean)\n",
    "\n",
    "selected_variables_clean = remove_correlated_task_variables(selected_variables_clean)\n",
    "\n",
    "3 impute data\n",
    "- missForest\n",
    "\n",
    "4 separate task & survey|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test EFA appropriateness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Sample size (participants:variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good enough for a pilot\n"
     ]
    }
   ],
   "source": [
    "#sample size\n",
    "ncases, nvars = task_df.shape\n",
    "sample_ratio = ncases / nvars\n",
    "if sample_ratio > 3:\n",
    "    print('good enough for a pilot')\n",
    "if sample_ratio >= 5:\n",
    "    print('reasonable for a full analysis, but not ideal')\n",
    "if sample_ratio >= 20:\n",
    "    print('good enough to publish with!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. bartlett test, want p < 0.5 (or 0.1, or 0.01 ...)\n",
    "\n",
    "from https://www.statisticshowto.datasciencecentral.com/bartletts-test/#BTs :\n",
    "\n",
    "Bartlett’s test for Sphericity compares your correlation matrix (a matrix of Pearson correlations) to the identity matrix. In other words, it checks if there is a redundancy between variables that can be summarized with some factors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39472.44094151645, 0.0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chi_square_value,p=factor_analyzer.calculate_bartlett_sphericity(task_df)\n",
    "chi_square_value, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "680469.5429067654\n",
      "0.0\n",
      "6620.585584576937\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "chi_square_value1, p1 = ss.bartlett(*[task_df[col].values for col in task_df.columns])\n",
    "print(chi_square_value1)\n",
    "print(p1)\n",
    "chi_square_value2, p2 = ss.bartlett(*task_df.values)\n",
    "print(chi_square_value2)\n",
    "print(p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Kaiser-Meyer-Olkin (KMO) Test, want a score >= 0.6\n",
    "\n",
    "from https://www.statisticshowto.datasciencecentral.com/kaiser-meyer-olkin/ :\n",
    "\n",
    "Kaiser-Meyer-Olkin (KMO) Test is a measure of how suited your data is for Factor Analysis. The test measures sampling adequacy for each variable in the model and for the complete model. The statistic is a measure of the proportion of variance among variables that might be common variance. The lower the proportion, the more suited your data is to Factor Analysis.\n",
    "\n",
    "KMO returns values between 0 and 1. A rule of thumb for interpreting the statistic:\n",
    "\n",
    "KMO values between 0.8 and 1 indicate the sampling is adequate.\n",
    "\n",
    "KMO values less than 0.6 indicate the sampling is not adequate and that remedial action should be taken. Some authors put this value at 0.5, so use your own judgment for values between 0.5 and 0.6.\n",
    "\n",
    "KMO Values close to zero means that there are large partial correlations compared to the sum of correlations. In other words, there are widespread correlations which are a large problem for factor analysis\n",
    "\n",
    "For reference, Kaiser put the following values on the results:\n",
    "\n",
    "* 0.00 to 0.49 unacceptable.\n",
    "* 0.50 to 0.59 miserable.\n",
    "* 0.60 to 0.69 mediocre.\n",
    "* 0.70 to 0.79 middling.\n",
    "* 0.80 to 0.89 meritorious.\n",
    "* 0.90 to 1.00 marvelous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrymj/anaconda3/lib/python3.7/site-packages/factor_analyzer/utils.py:248: UserWarning: The inverse of the variance-covariance matrix was calculated using the Moore-Penrose generalized matrix inversion, due to its determinant being at or very close to zero.\n",
      "  warnings.warn('The inverse of the variance-covariance matrix '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7569843212073796"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kmo_all,kmo_model=factor_analyzer.calculate_kmo(task_df)\n",
    "kmo_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other implementaiton of Bartlett and KMO\n",
    "https://github.com/Sarmentor/KMO-Bartlett-Tests-Python/blob/master/tests_correlation.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bartlett_sphericity(dataset, corr_method=\"pearson\"):\n",
    "    \n",
    "    r\"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : dataframe, mandatory (numerical or ordinal variables)\n",
    "        \n",
    "    corr_method : {'pearson', 'spearman'}, optional\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    out : namedtuple\n",
    "        The function outputs the test value (chi2), the degrees of freedom (ddl)\n",
    "        and the p-value.\n",
    "        It also delivers the n_p_ratio if the number of instances (n) divided \n",
    "        by the numbers of variables (p) is more than 5. A warning might be issued.\n",
    "        \n",
    "        Ex:\n",
    "        chi2:  410.27280642443156\n",
    "        ddl:  45.0\n",
    "        p-value:  8.73359410503e-61\n",
    "        n_p_ratio:    20.00\n",
    "        \n",
    "        Out: Bartlett_Sphericity_Test_Results(chi2=410.27280642443156, ddl=45.0, pvalue=8.7335941050291506e-61)\n",
    "    \n",
    "    References\n",
    "    ----------\n",
    "    \n",
    "    [1] Bartlett,  M.  S.,  (1951),  The  Effect  of  Standardization  on  a  chi  square  Approximation  in  Factor\n",
    "    Analysis, Biometrika, 38, 337-344.\n",
    "    [2] R. Sarmento and V. Costa, (2017)\n",
    "    \"Comparative Approaches to Using R and Python for Statistical Data Analysis\", IGI-Global.\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    illustration how to use the function.\n",
    "    \n",
    "    >>> bartlett_sphericity(survey_data, corr_method=\"spearman\")\n",
    "    chi2:  410.27280642443145\n",
    "    ddl:  45.0\n",
    "    p-value:  8.73359410503e-61\n",
    "    n_p_ratio:    20.00\n",
    "    C:\\Users\\Rui Sarmento\\Anaconda3\\lib\\site-packages\\spyderlib\\widgets\\externalshell\\start_ipython_kernel.py:75: \n",
    "    UserWarning: NOTE: we advise  to  use  this  test  only  if  the number of instances (n) divided by the number of variables (p) is lower than 5. Please try the KMO test, for example.\n",
    "    backend_o = CONF.get('ipython_console', 'pylab/backend', 0)\n",
    "    Out[12]: Bartlett_Sphericity_Test_Results(chi2=410.27280642443156, ddl=45.0, pvalue=8.7335941050291506e-61)\n",
    "    \"\"\"\n",
    "    \n",
    "    import numpy as np\n",
    "    import math as math\n",
    "    import scipy.stats as stats\n",
    "    import warnings as warnings\n",
    "    import collections\n",
    "\n",
    "    #Dimensions of the Dataset\n",
    "    n = dataset.shape[0]\n",
    "    p = dataset.shape[1]\n",
    "    n_p_ratio = n / p\n",
    "    \n",
    "    #Several Calculations\n",
    "    chi2 = - (n - 1 - (2 * p + 5) / 6) * math.log(np.linalg.det(dataset.corr(method=corr_method)))\n",
    "    #Freedom Degree\n",
    "    ddl = p * (p - 1) / 2\n",
    "    #p-value\n",
    "    pvalue = stats.chi2.pdf(chi2 , ddl)\n",
    "    \n",
    "    Result = collections.namedtuple(\"Bartlett_Sphericity_Test_Results\", [\"chi2\", \"ddl\", \"pvalue\"], rename=False)   \n",
    "    \n",
    "    #Output of the results - named tuple\n",
    "    result = Result(chi2=chi2,ddl=ddl,pvalue=pvalue) \n",
    "\n",
    "    \n",
    "    #Output of the function\n",
    "    if n_p_ratio > 5 :\n",
    "        print(\"n_p_ratio: {0:8.2f}\".format(n_p_ratio))\n",
    "        warnings.warn(\"NOTE: we advise  to  use  this  test  only  if  the number of instances (n) divided by the number of variables (p) is lower than 5. Please try the KMO test, for example.\")\n",
    "        \n",
    "    \n",
    "    return result\n",
    "\n",
    "def kmo(dataset_corr):\n",
    "    \n",
    "    import numpy as np\n",
    "    import math as math\n",
    "    import collections\n",
    "    \n",
    "    r\"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset_corr : ndarray\n",
    "        Array containing dataset correlation\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    out : namedtuple\n",
    "        The function outputs the test value (value), the test value per variable (per_variable)\n",
    "       \n",
    "        Ex:\n",
    "        Out[30]: \n",
    "        KMO_Test_Results(value=0.798844102413, \n",
    "        per_variable=\n",
    "        Q1     0.812160468405\n",
    "        Q2     0.774161264483\n",
    "        Q3     0.786819432663\n",
    "        Q4     0.766251123086\n",
    "        Q5     0.800579196084\n",
    "        Q6     0.842927745203 \n",
    "        Q7     0.792010173432 \n",
    "        Q8     0.862037322891\n",
    "        Q9     0.714795031915 \n",
    "        Q10    0.856497242574\n",
    "        dtype: float64)\n",
    "    \n",
    "    References\n",
    "    ----------    \n",
    "    [1] Kaiser, H. F. (1970). A second generation little jiffy. Psychometrika, 35(4), 401-415.\n",
    "    [2] Kaiser, H. F. (1974). An index of factorial simplicity. Psychometrika, 39(1), 31-36.\n",
    "    [3] R. Sarmento and V. Costa, (2017)\n",
    "    \"Comparative Approaches to Using R and Python for Statistical Data Analysis\", IGI-Global\n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    illustration how to use the function.\n",
    "    \n",
    "    >>> kmo_test(survey_data.corr(method=\"spearman\"))\n",
    "         \n",
    "        KMO_Test_Results(value=0.798844102413, \n",
    "        per_variable=\n",
    "        Q1     0.812160468405\n",
    "        Q2     0.774161264483\n",
    "        Q3     0.786819432663\n",
    "        Q4     0.766251123086\n",
    "        Q5     0.800579196084\n",
    "        Q6     0.842927745203 \n",
    "        Q7     0.792010173432 \n",
    "        Q8     0.862037322891\n",
    "        Q9     0.714795031915 \n",
    "        Q10    0.856497242574\n",
    "        dtype: float64) \n",
    "\"\"\"\n",
    "    \n",
    "    \n",
    "\n",
    "    #KMO Test\n",
    "    #inverse of the correlation matrix\n",
    "    corr_inv = np.linalg.inv(dataset_corr)\n",
    "    nrow_inv_corr, ncol_inv_corr = dataset_corr.shape\n",
    "    \n",
    "    #partial correlation matrix\n",
    "    A = np.ones((nrow_inv_corr,ncol_inv_corr))\n",
    "    for i in range(0,nrow_inv_corr,1):\n",
    "        for j in range(i,ncol_inv_corr,1):\n",
    "            #above the diagonal\n",
    "            A[i,j] = - (corr_inv[i,j]) / (math.sqrt(corr_inv[i,i] * corr_inv[j,j]))\n",
    "            #below the diagonal\n",
    "            A[j,i] = A[i,j]\n",
    "    \n",
    "    #transform to an array of arrays (\"matrix\" with Python)\n",
    "    dataset_corr = np.asarray(dataset_corr)\n",
    "        \n",
    "    #KMO value\n",
    "    kmo_num = np.sum(np.square(dataset_corr)) - np.sum(np.square(np.diagonal(dataset_corr)))\n",
    "    kmo_denom = kmo_num + np.sum(np.square(A)) - np.sum(np.square(np.diagonal(A)))\n",
    "    kmo_value = kmo_num / kmo_denom\n",
    "    \n",
    "    \n",
    "    kmo_j = [None]*dataset_corr.shape[1]\n",
    "    #KMO per variable (diagonal of the spss anti-image matrix)\n",
    "    for j in range(0, dataset_corr.shape[1]):\n",
    "        kmo_j_num = np.sum(dataset_corr[:,[j]] ** 2) - dataset_corr[j,j] ** 2\n",
    "        kmo_j_denom = kmo_j_num + np.sum(A[:,[j]] ** 2) - A[j,j] ** 2\n",
    "        kmo_j[j] = kmo_j_num / kmo_j_denom\n",
    "\n",
    "    \n",
    "    Result = collections.namedtuple(\"KMO_Test_Results\", [\"value\", \"per_variable\"])   \n",
    "    \n",
    "    #Output of the results - named tuple    \n",
    "    return Result(value=kmo_value,per_variable=kmo_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40151.519350792536 9730.0 0.0\n",
      "39345.32680860707 9730.0 0.0\n",
      "pearson = same as the factor analyzer!\n"
     ]
    }
   ],
   "source": [
    "chi2_spear,ddl_spear,pvalue_spear = bartlett_sphericity(task_df, corr_method=\"spearman\")\n",
    "print(chi2_spear,ddl_spear,pvalue_spear )\n",
    "chi2_son,ddl_son,pvalue_son = bartlett_sphericity(task_df, corr_method=\"pearson\")\n",
    "print(chi2_son,ddl_son,pvalue_son)\n",
    "print('pearson = same as the factor analyzer!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7720771290329701\n",
      "0.7576891243402228\n",
      "pearson = same as the factor analyzer!\n"
     ]
    }
   ],
   "source": [
    "dataset_corr_spear = task_df.corr(method=\"spearman\")\n",
    "value_spear,per_variable_spear = kmo(dataset_corr_spear)\n",
    "print(value_spear)\n",
    "\n",
    "dataset_corr_son = task_df.corr(method=\"pearson\")\n",
    "value_son,per_variable_son = kmo(dataset_corr_son)\n",
    "print(value_son)\n",
    "print('pearson = same as the factor analyzer!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform EFA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = skdec.FactorAnalysis(n_components = 5)\n",
    "fa.fit(task_df)\n",
    "z_scores = ss.zscore(fa.score_samples(task_df))\n",
    "# for idx, score in enumerate(fa.score_samples(task_df)):\n",
    "#     print(score, z_scores[idx])\n",
    "    \n",
    "fa.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = skdec.FactorAnalysis(svd_method='lapack').fit(task_df)\n",
    "transformed_factors = factors.transform(task_df)\n",
    "\n",
    "factors1 = skdec.FactorAnalysis(tol=1e-10).fit(task_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efa_df = pd.DataFrame(factors.components_, columns=task_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_df = pd.DataFrame(transformed_factors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors.score(task_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors1.score(task_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(factors1.get_covariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efa_df1 = pd.DataFrame(factors1.components_, columns=task_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efa_df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. using factor_analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = factor_analyzer.FactorAnalyzer(rotation=None)\n",
    "fa.fit(task_df) #140 gotten from output of FactorAnalysis().fit(task_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "efa_df2 = pd.DataFrame(fa.loadings_)\n",
    "efa_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DVs = task_df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for DV in DVs:\n",
    "    print(DV)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Figuring out transformed vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = pd.read_csv('meaningful_variables_clean.csv')\n",
    "del clean_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_df = pd.read_csv('meaningful_variables.csv')\n",
    "del orig_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## vars taken from .out as successfully or unsuccessfuly positively or negatively transformed into normal\n",
    "\n",
    "****************************************\n",
    "** Successfully transformed 31 positively skewed variables:\n",
    "adaptive_n_back.avg_rt\n",
    "adaptive_n_back.mean_load\n",
    "angling_risk_task_always_sunny.release_coef_of_variation\n",
    "attention_network_task.congruent_rt\n",
    "attention_network_task.neutral_rt\n",
    "bickel_titrator.hyp_discount_rate_large\n",
    "bickel_titrator.hyp_discount_rate_medium\n",
    "bickel_titrator.hyp_discount_rate_small\n",
    "choice_reaction_time.avg_rt\n",
    "columbia_card_task_hot.gain_sensitivity\n",
    "dietary_decision.health_sensitivity\n",
    "dospert_eb_survey.health_safety\n",
    "dospert_rt_survey.ethical\n",
    "hierarchical_rule.avg_rt\n",
    "kirby.hyp_discount_rate_large\n",
    "kirby.hyp_discount_rate_medium\n",
    "kirby.hyp_discount_rate_small\n",
    "local_global_letter.global_congruent_rt\n",
    "motor_selective_stop_signal.proactive_control_rt\n",
    "shift_task.model_beta\n",
    "simon.congruent_sd_rt\n",
    "simon.incongruent_sd_rt\n",
    "simple_reaction_time.avg_rt\n",
    "stim_selective_stop_signal.SSRT\n",
    "stop_signal.SSRT_low\n",
    "stop_signal.proactive_SSRT_speeding\n",
    "threebytwo.avg_rt\n",
    "threebytwo.cue_switch_cost_rt_100.0\n",
    "threebytwo.task_switch_cost_rt_900.0\n",
    "tower_of_london.avg_move_time\n",
    "writing_task.positive_probability\n",
    "****************************************\n",
    "****************************************\n",
    "Dropping 2 positively skewed data that could not be transformed successfully:\n",
    "dickman_survey.dysfunctional\n",
    "impulsive_venture_survey.impulsiveness\n",
    "****************************************\n",
    "****************************************\n",
    "** Successfully transformed 26 negatively skewed variables:\n",
    "attention_network_task.conflict_acc\n",
    "attention_network_task.orienting_acc\n",
    "columbia_card_task_cold.loss_sensitivity\n",
    "dietary_decision.taste_sensitivity\n",
    "dot_pattern_expectancy.AY-BY_acc\n",
    "dot_pattern_expectancy.BX-BY_acc\n",
    "dot_pattern_expectancy.BX-BY_rt\n",
    "dot_pattern_expectancy.acc\n",
    "go_nogo.acc\n",
    "holt_laury_survey.beta\n",
    "holt_laury_survey.prob_weighting\n",
    "local_global_letter.conflict_acc\n",
    "mpq_control_survey.control\n",
    "recent_probes.acc\n",
    "recent_probes.proactive_interference_acc\n",
    "shape_matching.acc\n",
    "shift_task.model_decay\n",
    "simon.acc\n",
    "simon.congruent_acc\n",
    "simon.incongruent_acc\n",
    "simon.simon_acc\n",
    "stim_selective_stop_signal.ignore_acc\n",
    "stroop.acc\n",
    "stroop.stroop_acc\n",
    "ten_item_personality_survey.agreeableness\n",
    "threebytwo.acc\n",
    "****************************************\n",
    "****************************************\n",
    "Dropping 8 negatively skewed data that could not be transformed successfully:\n",
    "psychological_refractory_period_two_choices.task1_acc\n",
    "choice_reaction_time.acc\n",
    "motor_selective_stop_signal.ignore_acc\n",
    "attention_network_task.acc\n",
    "psychological_refractory_period_two_choices.task2_acc\n",
    "information_sampling_task.Fixed_Win_acc\n",
    "directed_forgetting.acc\n",
    "local_global_letter.acc\n",
    "****************************************\n",
    "**************************************************\n",
    "Dropping 25 variables with correlations above 0.85\n",
    "**************************************************\n",
    "angling_risk_task_always_sunny.release_score\n",
    "angling_risk_task_always_sunny.keep_score\n",
    "attention_network_task.neutral_rt.logTr\n",
    "attention_network_task.congruent_rt.logTr\n",
    "attention_network_task.incongruent_rt\n",
    "dot_pattern_expectancy.avg_rt\n",
    "go_nogo.dprime\n",
    "hierarchical_rule.score\n",
    "kirby.hyp_discount_rate_medium.logTr\n",
    "kirby.percent_patient_large\n",
    "kirby.percent_patient\n",
    "kirby.percent_patient_small\n",
    "kirby.percent_patient_medium\n",
    "kirby.hyp_discount_rate_small.logTr\n",
    "local_global_letter.congruent_rt\n",
    "local_global_letter.global_congruent_rt.logTr\n",
    "local_global_letter.incongruent_harm_acc\n",
    "local_global_letter.incongruent_rt\n",
    "local_global_letter.local_congruent_rt\n",
    "probabilistic_selection.value_sensitivity\n",
    "simon.incongruent_acc.ReflogTr\n",
    "simon.congruent_avg_rt\n",
    "simon.incongruent_avg_rt\n",
    "stroop.congruent_rt\n",
    "stroop.incongruent_rt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "success_pos_vars = '''adaptive_n_back.avg_rt\n",
    "adaptive_n_back.mean_load\n",
    "angling_risk_task_always_sunny.release_coef_of_variation\n",
    "attention_network_task.congruent_rt\n",
    "attention_network_task.neutral_rt\n",
    "bickel_titrator.hyp_discount_rate_large\n",
    "bickel_titrator.hyp_discount_rate_medium\n",
    "bickel_titrator.hyp_discount_rate_small\n",
    "choice_reaction_time.avg_rt\n",
    "columbia_card_task_hot.gain_sensitivity\n",
    "dietary_decision.health_sensitivity\n",
    "dospert_eb_survey.health_safety\n",
    "dospert_rt_survey.ethical\n",
    "hierarchical_rule.avg_rt\n",
    "kirby.hyp_discount_rate_large\n",
    "kirby.hyp_discount_rate_medium\n",
    "kirby.hyp_discount_rate_small\n",
    "local_global_letter.global_congruent_rt\n",
    "motor_selective_stop_signal.proactive_control_rt\n",
    "shift_task.model_beta\n",
    "simon.congruent_sd_rt\n",
    "simon.incongruent_sd_rt\n",
    "simple_reaction_time.avg_rt\n",
    "stim_selective_stop_signal.SSRT\n",
    "stop_signal.SSRT_low\n",
    "stop_signal.proactive_SSRT_speeding\n",
    "threebytwo.avg_rt\n",
    "threebytwo.cue_switch_cost_rt_100.0\n",
    "threebytwo.task_switch_cost_rt_900.0\n",
    "tower_of_london.avg_move_time\n",
    "writing_task.positive_probability'''.split('\\n')\n",
    "\n",
    "fail_pos_vars = '''\n",
    "dickman_survey.dysfunctional\n",
    "impulsive_venture_survey.impulsiveness'''.split('\\n')\n",
    "\n",
    "success_neg_vars = '''\n",
    "attention_network_task.conflict_acc\n",
    "attention_network_task.orienting_acc\n",
    "columbia_card_task_cold.loss_sensitivity\n",
    "dietary_decision.taste_sensitivity\n",
    "dot_pattern_expectancy.AY-BY_acc\n",
    "dot_pattern_expectancy.BX-BY_acc\n",
    "dot_pattern_expectancy.BX-BY_rt\n",
    "dot_pattern_expectancy.acc\n",
    "go_nogo.acc\n",
    "holt_laury_survey.beta\n",
    "holt_laury_survey.prob_weighting\n",
    "local_global_letter.conflict_acc\n",
    "mpq_control_survey.control\n",
    "recent_probes.acc\n",
    "recent_probes.proactive_interference_acc\n",
    "shape_matching.acc\n",
    "shift_task.model_decay\n",
    "simon.acc\n",
    "simon.congruent_acc\n",
    "simon.incongruent_acc\n",
    "simon.simon_acc\n",
    "stim_selective_stop_signal.ignore_acc\n",
    "stroop.acc\n",
    "stroop.stroop_acc\n",
    "ten_item_personality_survey.agreeableness\n",
    "threebytwo.acc'''.split('\\n')\n",
    "\n",
    "fail_neg_vars = '''\n",
    "choice_reaction_time.acc\n",
    "motor_selective_stop_signal.ignore_acc\n",
    "attention_network_task.acc\n",
    "psychological_refractory_period_two_choices.task2_acc\n",
    "information_sampling_task.Fixed_Win_acc\n",
    "directed_forgetting.acc\n",
    "local_global_letter.acc'''.split('\\n')\n",
    "\n",
    "over_correlated_vars='''\n",
    "angling_risk_task_always_sunny.release_score\n",
    "angling_risk_task_always_sunny.keep_score\n",
    "attention_network_task.neutral_rt.logTr\n",
    "attention_network_task.congruent_rt.logTr\n",
    "attention_network_task.incongruent_rt\n",
    "dot_pattern_expectancy.avg_rt\n",
    "go_nogo.dprime\n",
    "hierarchical_rule.score\n",
    "kirby.hyp_discount_rate_medium.logTr\n",
    "kirby.percent_patient_large\n",
    "kirby.percent_patient\n",
    "kirby.percent_patient_small\n",
    "kirby.percent_patient_medium\n",
    "kirby.hyp_discount_rate_small.logTr\n",
    "local_global_letter.congruent_rt\n",
    "local_global_letter.global_congruent_rt.logTr\n",
    "local_global_letter.incongruent_harm_acc\n",
    "local_global_letter.incongruent_rt\n",
    "local_global_letter.local_congruent_rt\n",
    "probabilistic_selection.value_sensitivity\n",
    "simon.incongruent_acc.ReflogTr\n",
    "simon.congruent_avg_rt\n",
    "simon.incongruent_avg_rt\n",
    "stroop.congruent_rt\n",
    "stroop.incongruent_rt'''.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'orig_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7c1d1c99395e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0morig_colums\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0;34m[\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcol\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morig_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mall_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuccess_pos_vars\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfail_pos_vars\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msuccess_neg_vars\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mfail_neg_vars\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# for var in all_vars:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     if var != '':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'orig_df' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "orig_colums =  [col for col in orig_df.columns]\n",
    "\n",
    "all_vars = success_pos_vars + fail_pos_vars + success_neg_vars +fail_neg_vars\n",
    "# for var in all_vars:\n",
    "#     if var != '':\n",
    "#         if var in orig_colums:\n",
    "#             print('yay! ' + var + ' is in the original')\n",
    "#         else:\n",
    "#             print('BOOO! ' + var + ' is NOT in the original')\n",
    "            \n",
    "# for var in orig_colums:\n",
    "#     if var != '':\n",
    "#         if var in all_vars:\n",
    "#             print('yay! ' + var + 'has been transformed if necessary')\n",
    "#         else:\n",
    "#             print('BOOO! ' + var + ' was not touched!')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [col for col in clean_df.columns] #grab the columns of the \"cleaned\" \n",
    "\n",
    "print('SUCCESSFULL POSITIVELY SKEWED')\n",
    "for var in success_pos_vars:\n",
    "    if var not in over_correlated_vars:\n",
    "        tmp_var = var+'.logTr'\n",
    "        if tmp_var not in over_correlated_vars:\n",
    "            if tmp_var in columns:\n",
    "                print('yes! ' + tmp_var + ' has been kept as a column')\n",
    "            elif var in columns:\n",
    "                print('weirdly, ' + var + ' has been kept as a column, and not ' + tmp_var)\n",
    "            else:\n",
    "                print('no, ' + tmp_var + ' is no longer a column (for a mysterious reason!?)')\n",
    "                \n",
    "print('SUCCESSFUL NEGATIVELY SKEWED')\n",
    "for var in success_neg_vars:\n",
    "    if var not in over_correlated_vars:\n",
    "        tmp_var = var+'.ReflogTr'\n",
    "        if tmp_var not in over_correlated_vars:\n",
    "            if tmp_var in columns:\n",
    "                print('yes! ' + tmp_var + ' has been kept as a column')\n",
    "            elif var in columns:\n",
    "                print('weirdly, ' + var + ' has been kept as a column, and not ' + tmp_var)\n",
    "            else:\n",
    "                print('no, ' + tmp_var + ' is no longer a column (for a mysterious reason!?)')\n",
    "# for var in fail_pos_vars:\n",
    "#     tmp_var = var+'.logTr'\n",
    "#     if (var+'.logTr') in columns:\n",
    "#         print('yes!, ' + tmp_var + ' has been kept as a column')\n",
    "#     else:\n",
    "#         print('no, ' + tmp_var + ' is no longer a column (for a mysterious reason!?)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selected_variables_clean = transform_remove_skew(selected_variables)\n",
    "\n",
    "selected_variables_clean = remove_outliers(selected_variables_clean)\n",
    "\n",
    "selected_variables_clean = remove_correlated_task_variables(selected_variables_clean)\n",
    "\n",
    "selected_variables_clean.to_csv(path.join(directory, 'meaningful_variables_clean.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_remove_skew(data, threshold=1, \n",
    "                          positive_skewed=None,\n",
    "                          negative_skewed=None,\n",
    "                          verbose=True):\n",
    "    data = data.copy()\n",
    "    if positive_skewed is None:\n",
    "        positive_skewed = data.skew()>threshold\n",
    "    if negative_skewed is None:\n",
    "        negative_skewed = data.skew()<-threshold\n",
    "    positive_subset = data.loc[:,positive_skewed]\n",
    "    negative_subset = data.loc[:,negative_skewed]\n",
    "    # transform variables\n",
    "    # log transform for positive skew\n",
    "    shift = pd.Series(0, index=positive_subset.columns)\n",
    "    shift_variables = positive_subset.min()<=0\n",
    "    shift[shift_variables] -= (positive_subset.min()[shift_variables]-1)\n",
    "    positive_subset = np.log(positive_subset+shift)\n",
    "    # remove outliers\n",
    "    positive_tmp = remove_outliers(positive_subset)\n",
    "    successful_transforms = positive_subset.loc[:,abs(positive_tmp.skew())<threshold]\n",
    "    if verbose:\n",
    "        print('*'*40)\n",
    "        print('** Successfully transformed %s positively skewed variables:' % len(successful_transforms.columns))\n",
    "        print('\\n'.join(successful_transforms.columns))\n",
    "        print('*'*40)\n",
    "    dropped_vars = set(positive_subset)-set(successful_transforms)\n",
    "    # replace transformed variables\n",
    "    data.drop(positive_subset, axis=1, inplace = True)\n",
    "    successful_transforms.columns = [i + '.logTr' for i in successful_transforms]\n",
    "    if verbose:\n",
    "        print('*'*40)\n",
    "        print('Dropping %s positively skewed data that could not be transformed successfully:' % len(dropped_vars))\n",
    "        print('\\n'.join(dropped_vars))\n",
    "        print('*'*40)\n",
    "    data = pd.concat([data, successful_transforms], axis = 1)\n",
    "    # reflected log transform for negative skew\n",
    "    negative_subset = np.log(negative_subset.max()+1-negative_subset)\n",
    "    negative_tmp = remove_outliers(negative_subset)\n",
    "    successful_transforms = negative_subset.loc[:,abs(negative_tmp.skew())<threshold]\n",
    "    if verbose:\n",
    "        print('*'*40)\n",
    "        print('** Successfully transformed %s negatively skewed variables:' % len(successful_transforms.columns))\n",
    "        print('\\n'.join(successful_transforms.columns))\n",
    "        print('*'*40)\n",
    "    dropped_vars = set(negative_subset)-set(successful_transforms)\n",
    "    # replace transformed variables\n",
    "    data.drop(negative_subset, axis=1, inplace = True)\n",
    "    successful_transforms.columns = [i + '.ReflogTr' for i in successful_transforms]\n",
    "    if verbose:\n",
    "        print('*'*40)\n",
    "        print('Dropping %s negatively skewed data that could not be transformed successfully:' % len(dropped_vars))\n",
    "        print('\\n'.join(dropped_vars))\n",
    "        print('*'*40)\n",
    "    data = pd.concat([data, successful_transforms], axis=1)\n",
    "    return data.sort_index(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(data, quantile_range = 2.5):\n",
    "    '''Removes outliers more than 1.5IQR below Q1 or above Q3\n",
    "    '''\n",
    "    data = data.copy()\n",
    "    quantiles = data.apply(lambda x: x.dropna().quantile([.25,.5,.75])).T\n",
    "    lowlimit = np.array(quantiles.iloc[:,1] - quantile_range*(quantiles.iloc[:,2] - quantiles.iloc[:,0]))\n",
    "    highlimit = np.array(quantiles.iloc[:,1] + quantile_range*(quantiles.iloc[:,2] - quantiles.iloc[:,0]))\n",
    "    data_mat = data.values\n",
    "    data_mat[np.logical_or((data_mat<lowlimit), (data_mat>highlimit))] = np.nan\n",
    "    data = pd.DataFrame(data=data_mat, index=data.index, columns=data.columns)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_correlated_task_variables(data, threshold=.85):\n",
    "    tasks = np.unique([i.split('.')[0] for i in data.columns])\n",
    "    columns_to_remove = []\n",
    "    for task in tasks:\n",
    "        task_data = data.filter(regex = '^%s' % task)\n",
    "        corr_mat = task_data.corr().replace({1:0})\n",
    "        i=0\n",
    "        while True:\n",
    "            kept_indices = np.where(abs(corr_mat.iloc[:,i])<threshold)[0]\n",
    "            corr_mat = corr_mat.iloc[kept_indices,kept_indices]\n",
    "            i+=1\n",
    "            if i>=corr_mat.shape[0]:\n",
    "                break\n",
    "        columns_to_remove += list(set(task_data.columns)-set(corr_mat.columns))\n",
    "    print( '*' * 50)\n",
    "    print('Dropping %s variables with correlations above %s' % (len(columns_to_remove), threshold))\n",
    "    print( '*' * 50)\n",
    "    print('\\n'.join(columns_to_remove))\n",
    "    data = drop_vars(data,columns_to_remove)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_vars(data, drop_vars = [], saved_vars = []):\n",
    "    if len(drop_vars) == 0:\n",
    "        # variables that are calculated without regard to their actual interest\n",
    "        basic_vars = [\"\\.missed_percent$\",\"\\.acc$\",\"\\.avg_rt_error$\",\"\\.std_rt_error$\",\"\\.avg_rt$\",\"\\.std_rt$\"]\n",
    "        #unnecessary ddm params\n",
    "        ddm_vars = ['.*\\.(EZ|hddm)_(drift|thresh|non_decision).+$']\n",
    "        # variables that are of theoretical interest, but we aren't certain enough to include in 2nd stage analysis\n",
    "        exploratory_vars = [\"\\.congruency_seq\", \"\\.post_error_slowing$\"]\n",
    "        # task variables that are irrelevent to second stage analysis, either because they are correlated\n",
    "        # with other DV's or are just of no interest. Each row is a task\n",
    "        task_vars = [\"demographics\", # demographics\n",
    "                    \"(keep|release)_loss_percent\", # angling risk task\n",
    "                    \".first_order\", \"bis11_survey.total\", # bis11\n",
    "                    \"bis_bas_survey.BAS_total\", \n",
    "                    \"dietary_decision.prop_healthy_choice\", # dietary decision\n",
    "                    \"dot_pattern_expectancy.*errors\", # DPX errors\n",
    "                    \"eating_survey.total\", # eating total score\n",
    "                    \"five_facet_mindfulness_survey.total\", \n",
    "                    \"\\.risky_choices$\", \"\\.number_of_switches\", # holt and laury\n",
    "                    \"boxes_opened$\", # information sampling task\n",
    "                    \"_total_points$\", # IST\n",
    "                    \"\\.go_acc$\", \"\\.nogo_acc$\", \"\\.go_rt$\", \"go_nogo.*error.*\", #go_nogo\n",
    "                    \"discount_titrate.hyp_discount_rate\", \"discount_titrate.hyp_discount_rate_(glm|nm)\"  #delay discounting\n",
    "                    \"kirby.percent_patient\",\"kirby.hyp_discount_rate$\",  \"kirby.exp_discount.*\", \n",
    "                    \"\\.warnings$\", \"_notnow$\", \"_now$\", #kirby and delay discounting\n",
    "                    \"auc\", # bickel\n",
    "                    \"local_global_letter.*error.*\", # local global errors\n",
    "                    \"PRP_slowing\", # PRP_two_choices\n",
    "                    \"shape_matching.*prim.*\", # shape matching prime measures\n",
    "                    \"sensation_seeking_survey.total\", # SSS\n",
    "                    \"DDS\", \"DNN\", \"DSD\", \"SDD\", \"SSS\", \"DDD\", \"stimulus_interference_rt\", # shape matching\n",
    "                    \"shift_task.*errors\", \"shift_task.model_fit\", \"shift_task.conceptual_responses\", #shift task\n",
    "                    \"shift_task.fail_to_maintain_set\", 'shift_task.perseverative_responses', # shift task continued\n",
    "                     \"go_acc\",\"stop_acc\",\"go_rt_error\",\"go_rt_std_error\", \"go_rt\",\"go_rt_std\", # stop signal\n",
    "                     \"stop_rt_error\",\"stop_rt_error_std\",\"SS_delay\", \"^stop_signal.SSRT$\", # stop signal continue\n",
    "                     \"stop_signal.*errors\", \"inhibition_slope\", # stop signal continued\n",
    "                     \"stroop.*errors\", # stroop\n",
    "                     \"threebytwo.*inhibition\", # threebytwo\n",
    "                     \"num_correct\", \"weighted_performance_score\", # tower of london\n",
    "                     \"sentiment_label\" ,# writing task\n",
    "                     \"log_ll\", \"match_pct\", \"min_rss\", #fit indices\n",
    "                     \"num_trials\", \"num_stop_trials\"#num trials\n",
    "                    ]\n",
    "        drop_vars = basic_vars + exploratory_vars + task_vars + ddm_vars\n",
    "    drop_vars = '|'.join(drop_vars)\n",
    "    if len(saved_vars) > 0 :\n",
    "        saved_vars = '|'.join(saved_vars)\n",
    "        saved_columns = data.filter(regex=saved_vars)\n",
    "        dropped_data =  data.drop(data.filter(regex=drop_vars).columns, axis = 1)\n",
    "        final_data = dropped_data.join(saved_columns).sort_index(axis = 1)\n",
    "    else:\n",
    "        final_data = data.drop(data.filter(regex=drop_vars).columns, axis = 1)\n",
    "    return final_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      "** Successfully transformed 38 positively skewed variables:\n",
      "adaptive_n_back.mean_load\n",
      "angling_risk_task_always_sunny.release_coef_of_variation\n",
      "attention_network_task.conflict_rt\n",
      "attention_network_task.incongruent_rt\n",
      "attention_network_task.neutral_rt\n",
      "bickel_titrator.hyp_discount_rate_large\n",
      "bickel_titrator.hyp_discount_rate_medium\n",
      "bickel_titrator.hyp_discount_rate_small\n",
      "bis11_survey.Motor\n",
      "choice_reaction_time.avg_rt\n",
      "columbia_card_task_cold.gain_sensitivity\n",
      "columbia_card_task_hot.gain_sensitivity\n",
      "directed_forgetting.proactive_interference_acc\n",
      "dospert_eb_survey.health_safety\n",
      "dot_pattern_expectancy.AX_rt\n",
      "dot_pattern_expectancy.BX_rt\n",
      "dot_pattern_expectancy.avg_rt\n",
      "holt_laury_survey.beta\n",
      "kirby.hyp_discount_rate_large\n",
      "kirby.hyp_discount_rate_medium\n",
      "kirby.hyp_discount_rate_small\n",
      "motor_selective_stop_signal.ignore_rt_error_std\n",
      "motor_selective_stop_signal.proactive_control_rt\n",
      "shape_matching.avg_rt\n",
      "shift_task.model_beta\n",
      "simon.avg_rt\n",
      "simon.congruent_avg_rt\n",
      "simon.congruent_sd_rt\n",
      "simon.incongruent_avg_rt\n",
      "simon.incongruent_sd_rt\n",
      "simple_reaction_time.avg_rt\n",
      "stim_selective_stop_signal.ignore_rt_error_std\n",
      "stop_signal.SSRT_high\n",
      "stroop.congruent_rt\n",
      "threebytwo.avg_rt\n",
      "threebytwo.cue_switch_cost_rt_100.0\n",
      "threebytwo.cue_switch_cost_rt_900.0\n",
      "tower_of_london.avg_move_time\n",
      "****************************************\n",
      "****************************************\n",
      "Dropping 2 positively skewed data that could not be transformed successfully:\n",
      "dickman_survey.dysfunctional\n",
      "impulsive_venture_survey.impulsiveness\n",
      "****************************************\n",
      "****************************************\n",
      "** Successfully transformed 22 negatively skewed variables:\n",
      "adaptive_n_back.acc\n",
      "attention_network_task.acc\n",
      "attention_network_task.conflict_acc\n",
      "choice_reaction_time.acc\n",
      "columbia_card_task_hot.loss_sensitivity\n",
      "dot_pattern_expectancy.AY-BY_acc\n",
      "dot_pattern_expectancy.BX-BY_acc\n",
      "dot_pattern_expectancy.BX-BY_rt\n",
      "go_nogo.acc\n",
      "local_global_letter.acc\n",
      "local_global_letter.conflict_acc\n",
      "local_global_letter.congruent_facilitation_acc\n",
      "local_global_letter.incongruent_harm_acc\n",
      "mpq_control_survey.control\n",
      "recent_probes.acc\n",
      "selection_optimization_compensation_survey.optimization\n",
      "shift_task.model_decay\n",
      "simon.incongruent_acc\n",
      "simon.simon_acc\n",
      "stim_selective_stop_signal.ignore_acc\n",
      "stroop.stroop_acc\n",
      "ten_item_personality_survey.conscientiousness\n",
      "****************************************\n",
      "****************************************\n",
      "Dropping 11 negatively skewed data that could not be transformed successfully:\n",
      "information_sampling_task.Fixed_Win_acc\n",
      "dot_pattern_expectancy.acc\n",
      "simon.acc\n",
      "stroop.acc\n",
      "simon.congruent_acc\n",
      "holt_laury_survey.prob_weighting\n",
      "directed_forgetting.acc\n",
      "psychological_refractory_period_two_choices.task1_acc\n",
      "psychological_refractory_period_two_choices.task2_acc\n",
      "motor_selective_stop_signal.ignore_acc\n",
      "shape_matching.acc\n",
      "****************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrymj/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in less\n",
      "  if __name__ == '__main__':\n",
      "/Users/henrymj/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in greater\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "selected_vars_clean = transform_remove_skew(orig_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38\n",
      "22\n"
     ]
    }
   ],
   "source": [
    "print(len([i for i in selected_vars_clean.columns if '.logTr' in i]))\n",
    "print(len([i for i in selected_vars_clean.columns if '.ReflogTr' in i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "failed_dropped = '''\n",
    "impulsive_venture_survey.impulsiveness\n",
    "dickman_survey.dysfunctional\n",
    "holt_laury_survey.prob_weighting\n",
    "simon.congruent_acc\n",
    "shape_matching.acc\n",
    "information_sampling_task.Fixed_Win_acc\n",
    "psychological_refractory_period_two_choices.task2_acc\n",
    "motor_selective_stop_signal.ignore_acc\n",
    "dot_pattern_expectancy.acc\n",
    "stroop.acc\n",
    "psychological_refractory_period_two_choices.task1_acc\n",
    "simon.acc\n",
    "directed_forgetting.acc'''.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in failed_dropped:\n",
    "    if var in selected_vars_clean.columns:\n",
    "        print(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selected_variables_clean = transform_remove_skew(selected_variables)\n",
    "\n",
    "selected_variables_clean = remove_outliers(selected_variables_clean)\n",
    "\n",
    "selected_variables_clean = remove_correlated_task_variables(selected_variables_clean)\n",
    "\n",
    "selected_variables_clean.to_csv(path.join(directory, 'meaningful_variables_clean.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/henrymj/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in less\n",
      "  if __name__ == '__main__':\n",
      "/Users/henrymj/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:9: RuntimeWarning: invalid value encountered in greater\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      "Dropping 24 variables with correlations above 0.85\n",
      "**************************************************\n",
      "angling_risk_task_always_sunny.release_score\n",
      "angling_risk_task_always_sunny.keep_score\n",
      "attention_network_task.incongruent_rt.logTr\n",
      "attention_network_task.congruent_rt\n",
      "attention_network_task.neutral_rt.logTr\n",
      "dot_pattern_expectancy.avg_rt.logTr\n",
      "go_nogo.dprime\n",
      "hierarchical_rule.score\n",
      "kirby.percent_patient_large\n",
      "kirby.percent_patient\n",
      "kirby.percent_patient_small\n",
      "kirby.percent_patient_medium\n",
      "kirby.hyp_discount_rate_medium.logTr\n",
      "local_global_letter.incongruent_harm_acc.ReflogTr\n",
      "local_global_letter.global_congruent_rt\n",
      "local_global_letter.congruent_rt\n",
      "local_global_letter.local_congruent_rt\n",
      "local_global_letter.incongruent_rt\n",
      "probabilistic_selection.avoid_trial_rt\n",
      "probabilistic_selection.value_sensitivity\n",
      "simon.congruent_avg_rt.logTr\n",
      "simon.incongruent_avg_rt.logTr\n",
      "stroop.incongruent_rt\n",
      "stroop.congruent_rt.logTr\n"
     ]
    }
   ],
   "source": [
    "selected_vars_clean = remove_outliers(selected_vars_clean)\n",
    "selected_vars_clean = remove_correlated_task_variables(selected_vars_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "over_correlated_vars = '''angling_risk_task_always_sunny.keep_score\n",
    "angling_risk_task_always_sunny.release_score\n",
    "attention_network_task.incongruent_rt.logTr\n",
    "attention_network_task.congruent_rt\n",
    "attention_network_task.neutral_rt.logTr\n",
    "dot_pattern_expectancy.avg_rt.logTr\n",
    "go_nogo.dprime\n",
    "hierarchical_rule.score\n",
    "kirby.percent_patient_medium\n",
    "kirby.percent_patient_small\n",
    "kirby.hyp_discount_rate_medium.logTr\n",
    "kirby.percent_patient_large\n",
    "kirby.percent_patient\n",
    "local_global_letter.local_congruent_rt\n",
    "local_global_letter.global_congruent_rt\n",
    "local_global_letter.incongruent_harm_acc.ReflogTr\n",
    "local_global_letter.congruent_rt\n",
    "local_global_letter.incongruent_rt\n",
    "probabilistic_selection.avoid_trial_rt\n",
    "probabilistic_selection.value_sensitivity\n",
    "simon.congruent_avg_rt.logTr\n",
    "simon.incongruent_avg_rt.logTr\n",
    "stroop.congruent_rt.logTr\n",
    "stroop.incongruent_rt'''.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in over_correlated_vars:\n",
    "    if var in selected_vars_clean.columns:\n",
    "        print(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rpy2.robjects\n",
    "from rpy2.robjects import pandas2ri, Formula\n",
    "from rpy2.robjects.packages import importr\n",
    "from selfregulation.utils.utils import get_info\n",
    "pandas2ri.activate()\n",
    "\n",
    "def missForest(data):\n",
    "#     try:\n",
    "#         missForest = importr('missForest', lib_loc='/Library/Frameworks/R.framework/Versions/3.6/Resources/library')\n",
    "#     except:\n",
    "#         missForest = importr('missForest', lib_loc='/Library/Frameworks/R.framework/Resources/library')\n",
    "    missForest = importr('missForest', lib_loc='/Library/Frameworks/R.framework/Versions/3.6/Resources/library')\n",
    "    data_complete, error = missForest.missForest(data)\n",
    "    imputed_df = pd.DataFrame(np.matrix(data_complete).T, index=data.index, columns=data.columns)\n",
    "    return imputed_df, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "RRuntimeError",
     "evalue": "Error in library.dynam(lib, package, package.lib) : \n  shared object ‘randomForest.dylib’ not found\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRRuntimeError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-d9e362a1ab34>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mselected_variables_imputed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_vars_clean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-1b338205b121>\u001b[0m in \u001b[0;36mmissForest\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#     except:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#         missForest = importr('missForest', lib_loc='/Library/Frameworks/R.framework/Resources/library')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mmissForest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'missForest'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib_loc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/Library/Frameworks/R.framework/Versions/3.6/Resources/library'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mdata_complete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmissForest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissForest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mimputed_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_complete\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/rpy2/robjects/packages.py\u001b[0m in \u001b[0;36mimportr\u001b[0;34m(name, lib_loc, robject_translations, signature_translation, suppress_messages, on_conflict, symbol_r2python, symbol_check_after, data)\u001b[0m\n\u001b[1;32m    451\u001b[0m     if _package_has_namespace(rname, \n\u001b[1;32m    452\u001b[0m                               _system_file(package = rname)):\n\u001b[0;32m--> 453\u001b[0;31m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0mversion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_namespace_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0mexported_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_namespace_exports\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRRuntimeError\u001b[0m: Error in library.dynam(lib, package, package.lib) : \n  shared object ‘randomForest.dylib’ not found\n"
     ]
    }
   ],
   "source": [
    "selected_variables_imputed, error = missForest(selected_vars_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Options cannot be set once R has been initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-6c3952371c1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrinterface\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrinterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_initoptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb'rpy2'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb'--no-save'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb'--no-restore'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mb'--quiet'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrpy2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrobjects\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpackages\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimportr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimportr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'base'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_libPaths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Options cannot be set once R has been initialized."
     ]
    }
   ],
   "source": [
    "import rpy2.rinterface\n",
    "rpy2.rinterface.set_initoptions((b'rpy2', b'--no-save', b'--no-restore', b'--quiet'))\n",
    "from rpy2.robjects.packages import importr\n",
    "base = importr('base')\n",
    "print(base._libPaths())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
